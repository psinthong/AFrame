import pandas as pd
import configparser
import os
import json
from abc import ABC, abstractmethod


class Connector(ABC):

    def __init__(self, server_address=None, config_file_path=None, **kwargs):
        self._server_address = server_address
        self._config_file_path = config_file_path

    @property
    def server_address(self):
        return self._server_address

    @abstractmethod
    def connect(self, config_file):
        pass

    @abstractmethod
    def send_request(self, query):
        pass

    def get_config_queries(self):
        queries = {}
        config = configparser.ConfigParser()
        config.read(os.path.join(os.path.abspath(os.path.dirname(__file__)), 'conf', self._config_file_path))
        for section in config.sections():
            for (key, value) in config.items(section):
                queries[key] = value.replace('\n', '\r\n')
        return queries

    def get_collection(self, dataverse, dataset):
        pass

    def get_view(self, dataverse, dataset):
        pass

    def to_collection(self, subquery, namespace, collection, name, query=False):
        pass

    def to_view(self, subquery, namespace, collection, name, query=False):
        pass

    def to_transformation(self, subquery, namespace, collection, name, query=False):
        pass

    def drop_collection(self, namespace, collection):
        pass

    def drop_view(self, namespace, collection):
        pass

    def drop_transformation(self, namespace, name):
        pass


class AsterixConnector(Connector):
    def __init__(self, server_address="http://localhost:19002", config_file_path="sql_pp.ini"):
        self._db = self.connect(server_address)
        Connector.__init__(self, server_address=self._db, config_file_path=config_file_path)

    def connect(self, server_address):
        import urllib.parse
        if not urllib.parse.urlparse(server_address).scheme:
            server_address = "http://" + server_address
        return server_address

    def send_request(self, query):
        import urllib.parse
        import urllib.request
        import urllib.error
        # from pandas.io import json

        host = self._server_address + '/query/service'
        data = dict()
        data['statement'] = query + ';'
        data = urllib.parse.urlencode(data).encode('utf-8')
        try:
            handler = urllib.request.urlopen(host, data)
            result = json.loads(handler.read())
            result = result['results']
            df = pd.DataFrame(result)
            if '_uuid' in df.columns:
                df.drop('_uuid', axis=1, inplace=True)
            return df

        except urllib.error.URLError as e:
            raise Exception('The following error occured: %s. Please check if AsterixDB is running.' % str(e.reason))

    def submit(self, query: str):
        import urllib.parse
        import urllib.request
        import urllib.error
        host = self.server_address+'/query/service'
        data = dict()
        data['statement'] = query
        data = urllib.parse.urlencode(data).encode('utf-8')
        with urllib.request.urlopen(host, data) as handler:
            result = json.loads(handler.read())
            return result['status']

    def to_collection(self, subquery, namespace, collection, name, query=False):
        new_query = 'CREATE TYPE $namespace._internalType IF NOT EXISTS AS OPEN{ _uuid: uuid};\n' \
                    'CREATE DATASET $namespace.$name(_internalType) PRIMARY KEY _uuid autogenerated;\n' \
                    'INSERT INTO $namespace.$name SELECT VALUE ($subquery);'
        new_query = new_query.replace('$namespace', namespace).replace('$name', name).replace('$subquery', subquery)
        if query:
            return new_query
        self.submit(new_query)
        return AsterixConnector(self.server_address)

    def to_view(self, subquery, namespace, collection, name, query=False):
        new_query = 'CREATE FUNCTION $namespace.$name(){\n$subquery};'
        new_query = new_query.replace('$namespace', namespace).replace('$name', name).replace('$subquery', subquery)
        if query:
            return new_query
        self.submit(new_query)
        return AsterixConnector(self.server_address)

    def to_transformation(self, subquery, namespace, collection, name, query=False):
        template = 'SELECT VALUE t FROM $namespace.$collection t'
        template = template.replace('$namespace', namespace).replace('$collection', collection)
        new_q = 'SELECT VALUE t FROM to_array(r) AS t'
        subquery = subquery.replace(template, new_q)
        new_query = 'CREATE OR REPLACE FUNCTION $namespace.$name(r){\n($subquery)[0]};'
        new_query = new_query.replace('$subquery', subquery).replace('$namespace', namespace).replace('$name', name)
        if query:
            return new_query
        self.submit(new_query)
        return AsterixConnector(self.server_address)

    def drop_collection(self, namespace, collection):
        drop_query = 'DROP DATASET {}.{};'.format(namespace, collection)
        return self.submit(drop_query)

    def drop_view(self, namespace, collection):
        drop_query = 'DROP FUNCTION {}.{}@0;'.format(namespace, collection)
        return self.submit(drop_query)

    def drop_transformation(self, namespace, name):
        drop_query = 'DROP FUNCTION {}.{}@1;'.format(namespace, name)
        return self.submit(drop_query)

    def get_view(self, dataverse, dataset):
        return '{}.{}();'.format(dataverse, dataset)

    def get_window(self, window):
        over = ''
        if window is not None:
            if window.part() is not None:
                over += 'PARTITION BY t.%s ' % window._part
            if window.ord() is not None:
                over += 'ORDER BY t.%s ' % window._ord
            if window.rows() is not None:
                frame = window.rows()
                if isinstance(frame, tuple):
                    start = frame[0]
                    end = frame[1]
                    start_str = ''
                    end_str = ''
                    if start == 0:
                        start_str = 'CURRENT ROW'
                    elif start != 0:
                        start_str = '%d PRECEDING' % (start * -1)
                    if end == 0:
                        end_str = 'CURRENT ROW'
                    elif end != 0:
                        end_str = '%d FOLLOWING' % end
                    rows = 'ROWS BETWEEN %s AND %s' % (start_str, end_str)
                    over += rows
        # else:
        #     over += 'ORDER BY t.%s ' % on
        # return 'OVER(%s)' % over
        return over


class SQLConnector(Connector):

    def __init__(self, server_address=None, config_file_path="sql.ini", engine=None):
        import sqlalchemy
        Connector.__init__(self, server_address, config_file_path)
        if isinstance(engine, sqlalchemy.engine.Engine):
            self._db = engine
        else:
            self._db = self.connect(db_str=self.server_address)

    def connect(self, db_str):
        from sqlalchemy import create_engine
        return create_engine(db_str)

    def send_request(self, query):
        result_obj = self._db.execute(query)
        results = result_obj.fetchall()
        return pd.DataFrame(results, columns=result_obj.keys())


class MongoConnector(Connector):

    def __init__(self, server_address=None, config_file_path="mongo.ini", engine=None):
        from pymongo import MongoClient
        Connector.__init__(self, server_address, config_file_path)
        if isinstance(engine, MongoClient):
            self._db = engine
        else:
            self._db = self.connect(db_str=self.server_address)

    def connect(self, db_str):
        from pymongo import MongoClient
        return MongoClient(db_str)

    def send_request(self, query):
        # drop_id = ', { "$project": { "_id": 0 } }'
        # if query[0] == ',':
        #     query = query[1:]
        # query += drop_id
        p_lst = [json.loads(s.strip(',')) for s in query.split('\r\n')]
        results = list(self._db.aggregate(pipeline=p_lst, allowDiskUse=True))
        return pd.DataFrame(results)

    def get_collection(self, dataverse, dataset):
        from pymongo.collection import Collection
        if not isinstance(self._db, Collection):
            self._db = self._db[dataverse][dataset]
        return None

    def to_collection(self, subquery, namespace, collection, name, query=False):
        new_query = '$subquery,\r\n{ "$out": "$name" }'
        new_query = new_query.replace('$name', name).replace('$subquery', subquery)
        if query:
            return new_query
        self.send_request(new_query)
        return MongoConnector(self.server_address)

    def to_view(self, subquery, namespace, collection, name, query=False):
        from pymongo import MongoClient

        pipeline = [json.loads(s.strip(',')) for s in subquery.split('\r\n')]
        db = MongoClient(self.server_address)
        create_view = {}
        create_view['create'] = "{}".format(name)
        create_view['viewOn'] = "{}".format(collection)
        create_view['pipeline'] = pipeline
        if query:
            return create_view
        db[namespace].command(create_view)
        return MongoConnector(self.server_address)

    def drop_collection(self, namespace, collection):
        self._db.drop()
        return 'success'

    get_view = get_collection

    drop_view = drop_collection


class CypherConnector(Connector):

    def __init__(self, uri='http://localhost:7474', config_file_path="cypher.ini", username=None, password=None):
        Connector.__init__(self, uri, config_file_path)
        self._db = self.connect(username=username,password=password)
        self._uri = uri
        self._username = username
        self._password = password

    def connect(self, username, password):
        from py2neo import Graph
        return Graph(self.server_address, username=username, password=password)

    def send_request(self, query):
        import pandas as pd
        formatted_q = query.replace('\r\n','\n')
        results = self._db.run(formatted_q).to_data_frame()
        if len(results['t']) > 0:
            if isinstance(results['t'][0], dict):
                df = pd.DataFrame.from_records(results['t'])
            else:
                df = pd.DataFrame(results['t'])
            return df

    def get_view(self, dataverse, dataset):
        query = 'MATCH (n:_View) WHERE n.namespace="{}" AND n.collection="{}" RETURN n.query AS n'.format(dataverse, dataset)
        original_q = self._db.run(query).to_data_frame()['n'][0]
        return original_q

    def to_view(self, subquery, namespace, collection, name, query=False):
        to_view_q = 'CREATE (n:_View { `namespace`: "'+namespace+'", collection: "'+name+'", query:"'+subquery+'"})'
        if query:
            return to_view_q
        self._db.run(to_view_q)
        return CypherConnector(uri=self._uri, username=self._username, password=self._password)

    def drop_view(self, namespace, collection):
        query = 'MATCH (n:_View) WHERE n.namespace = "{}" AND n.collection = "{}" DELETE n'.format(namespace, collection)
        self._db.run(query)
        return 'success'


class InfluxDBConnector(Connector):

    def __init__(self, server_address=None, config_file_path="flux.ini", token=None, org=None):

        Connector.__init__(self, server_address, config_file_path)
        self._db = self.connect(url=self.server_address, token=token)
        self._org = org

    def connect(self, url, token):
        from influxdb_client import InfluxDBClient
        client = InfluxDBClient(url=url, token=token)
        return client

    def send_request(self, query):
        internal_cols = ['result', 'table', '_start', '_stop', '_value', '_field', '_measurement']
        result_obj = self._db.query_api().query_data_frame(query, org=self._org)
        if isinstance(result_obj, pd.DataFrame):
            return result_obj.drop(internal_cols, axis=1, errors='ignore')
        else:
            return pd.concat(result_obj).drop(internal_cols, axis=1, errors='ignore')

    def submit(self, query):
        internal_cols = ['result', 'table', '_start', '_stop', '_value', '_field', '_measurement']
        result_obj = self._db.query_api().query(query,org=self._org)
        if isinstance(result_obj, pd.DataFrame):
            return result_obj.drop(internal_cols, axis=1, errors='ignore')
        else:
            return pd.concat(result_obj).drop(internal_cols, axis=1, errors='ignore')

    def to_collection(self, subquery, namespace, collection, name, query=False):
        new_query = '$subquery\n' \
                    '|> to(bucket:"$name", org:"$namespace")'
        new_query = new_query.replace('$namespace', namespace).replace('$name', name).replace('$subquery', subquery)
        if query:
            return new_query
        self.send_request(new_query)
        return AsterixConnector(self.server_address)

    def get_window(self, window):
        over = ''
        if window.rows() is not None:
            frame = window.rows()
            if isinstance(frame, str):
                period = 'period: %s' % (frame)
                over += period
        return over


class CBAnalyticsConnector(Connector):

    def __init__(self, server_address, username, password, config_file_path="sql_pp.ini"):

        Connector.__init__(self, server_address, config_file_path)
        self._db = server_address+'/analytics/service'
        self._username = username
        self._password = password

    def connect(self, server_address, username=None, password=None):
        import requests
        data = {'statement': 'select 1;'}
        return requests.get(server_address, params=data, auth=(username, password))

    def send_request(self, query):
        import requests
        data = {'statement': query}
        results = requests.post(self._db, data=data, auth=(self._username, self._password))
        json_result = results.json()
        if results.status_code == 200:
            return pd.DataFrame(json_result['results'])
        else:
            raise ValueError(json_result['errors'][0]['msg'])

    def to_view(self, subquery, namespace, collection, name, query=False):
        new_query = 'CREATE FUNCTION $namespace.$name(){\n$subquery};'
        new_query = new_query.replace('$namespace', namespace).replace('$name', name).replace('$subquery', subquery)
        if query:
            return new_query
        self.send_request(new_query)
        return CBAnalyticsConnector(self.server_address, self._username, self._password)

    def to_transformation(self, subquery, namespace, collection, name, query=False):
        template = 'SELECT VALUE t FROM $namespace.$collection t'
        template = template.replace('$namespace', namespace).replace('$collection', collection)
        new_q = 'SELECT VALUE t FROM to_array(r) AS t'
        subquery = subquery.replace(template, new_q)
        new_query = 'CREATE FUNCTION $namespace.$name(r){\n($subquery)[0]};'
        new_query = new_query.replace('$subquery', subquery).replace('$namespace', namespace).replace('$name', name)
        if query:
            return new_query
        self.send_request(new_query)
        return CBAnalyticsConnector(self.server_address, self._username, self._password)

    def drop_view(self, namespace, collection):
        drop_query = 'DROP FUNCTION {}.{}@0;'.format(namespace, collection)
        return self.send_request(drop_query)

    def drop_transformation(self, namespace, name):
        drop_query = 'DROP FUNCTION {}.{}@1;'.format(namespace, name)
        return self.send_request(drop_query)

    def get_view(self, dataverse, dataset):
        return '{}.{}();'.format(dataverse, dataset)
